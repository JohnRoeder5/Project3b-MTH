
---
title: "Project3b-MTH"
author: "John Roeder, Chris He, Malakai Vetock"
date: "2024-04-23"
date: "`r Sys.Date()`"
output: pdf_document
Date Created: Tuesday, 2024/04/23, 14:45
Date Modified: Monday, 2024/04/23, 14:50
class: MTH362 Statistical Modeling
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE) 
```

## Introduction

The dataset comes from FiveThirtyEight, a statistics focused news website that reported on sports, politics, and science.It contains the information of 233 ads from 10 brands that aired the most ad spots in 21 Superbowls ranging from 2000 to 2021 on the social media platform known as YouTube. The research question that is being investigated is "What factors may lead to a popular commercial during the super bowl? Given that the Superbowl is one of the most watched events each year with the most expensive ad slots, it makes sense to investigate factors that impact popularity and perception of money spent on ads during the Superbowl.

```{r}
library(tidyverse)
library(tidyr)
library(leaps)
youtube <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')
```

The dataset contains 247 observations of 25 variables. A derived boolean variable called `popular` was created using `view_count`, using an arbitrarily selected limit of 200,000 as a marker for a popular commercial video. As commercials ran during the Superbowl are expensive, it makes sense to set the standards of `popular` higher rather than lower. `popular` will be the response variable investigated in this project.


```{r}
# preprocessing 
num_columns <- ncol(youtube)
print(paste("Total Columns: ", num_columns))

#get col names 
#also check for nans
columns =colnames(youtube)
print(columns)
nas <- colSums(is.na(youtube))

#find cols w nans
cols_w_nas <- names(nas[nas > 0])
print("cols w Nans")
print(cols_w_nas)

#get the total missing for each col and print
missing_counts <- colSums(is.na(youtube))
print(missing_counts)
```
The preprocessing steps that we took for this dataset started off with checking for NaN cell values in the columns of the dataset. It can be seen that there are some columns like the view count, kind, and etag column that have very few missing cell values, but also columns like thumbnail and description that are missing quite a few cell values. Given the nature of this dataset, we felt that it was reasonable to fill the numeric cells with the median value for that individual column. replacing the NAN values happens in the chunk below.



```{r}

#for numeric cols w emptys
numeric_cols <- c("like_count", "dislike_count", "favorite_count", "comment_count", "view_count")
for (col in numeric_cols) {
  median_value <- median(youtube[[col]], na.rm = TRUE)  
  youtube[[col]][is.na(youtube[[col]])] <- median_value 
}

youtube <- youtube[!is.na(youtube$published_at), ]

head(youtube)

#check again to see if fixes worked. 
missing_counts <- colSums(is.na(youtube))
print(missing_counts)

```

```{r}
#change to binary features
#remove the unused features

youtube <- youtube %>%
  mutate_if(is.logical, as.integer)

youtube <- subset(youtube, select = -c(brand,
                                  superbowl_ads_dot_com_url,
                                  youtube_url,
                                  id,
                                  kind,
                                  etag,
                                  published_at,
                                  title,
                                  description,
                                  thumbnail,
                                  channel_title,
                                  category_id, 
                                  favorite_count))
youtube$popular<- with(youtube,ifelse(view_count>200000,1,0))

head(youtube)
```
Our last step of datapreprocessing is removing the unwanted columns. Columns like superbowl_ads_dot_com_url, youtube_url, id,kind, etag, published_at, title, description, thumbnail, channel_title, category_id, and favorite_count are not necessary for our goal of predicting what makes a popular superbowl ad. These columns were mostly text data that would not work for our chosen model type of logistic regression. 


## EDA


```{r}
#EDA plots 


year_viewcount <- data %>%
  group_by(year) %>%
  summarise(avg_view_count = mean(view_count))
#avg view count by year
ggplot(year_viewcount, aes(x = as.factor(year), y = avg_view_count)) +
  geom_bar(stat = "identity", fill = "lightgreen", color = "black") +
  labs(title = "AVG View Count by Year",
       x = "Year",
       y = "AVG View Count") +
  theme_minimal()


commercial_viewcount <- data %>%
  summarise(
    funny_viewcount = sum(view_count * funny),
    show_product_quickly_viewcount = sum(view_count * show_product_quickly),
    patriotic_viewcount = sum(view_count * patriotic),
    celebrity_viewcount = sum(view_count * celebrity),
    danger_viewcount = sum(view_count * danger),
    animals_viewcount = sum(view_count * animals),
    use_sex_viewcount = sum(view_count * use_sex)
  ) %>%
  pivot_longer(cols = everything(), 
               names_to = "commercial_type", values_to = "total_viewcount")


ggplot(commercial_viewcount, aes(x = commercial_type, y = total_viewcount)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Total View by commercial type",
       x = "Commercial type",
       y = "total view count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(data=youtube,aes(x=popular))+geom_bar(fill= "skyblue")+labs(title="Popular plot")


```
The above plot shows the average viewership of superbowl ads by year. 2012 had the highest average viewership followed by 2017 and 2020. This provides a visual of the relative increase in average viewership per superbowl ad. The total view by commercial type plot shows the total view counts across the different types of commercials. It can clearly be seen that the funny commercials and commercials that show products quickly have performed well in the past based on total view counts. Lastly the popular plot shows the counts of popular and unpopular superbowl commercials determined by our cutoff value of greater than 200,000 views. There are disproportionately more unpopular commercials as opposed to popular commercials per our metric. 





## Model

```{r}
logit.reg<- glm(popular ~ funny+ show_product_quickly+patriotic+celebrity+danger+animals+use_sex+like_count+dislike_count,data=youtube)
summary(logit.reg)
```

Like count and show product quickly were the only two significant predictors in the model with a p value below 0.05.  The intercept had a p value of 0.09 and was deemed significant enough to be put into the model, resulting in the following model.

$logit(p)=0.1195+1.275\text{show_product_quickly}+8.454e^{-06}\text{like_count}$


## Analysis


```{r}
ggplot(data=youtube,aes(x=like_count,y=popular))+geom_jitter(alpha=0.5, height=0.1) + geom_smooth(method='glm', method.args=list(family='binomial'), se=TRUE) + labs(x='like_count', y='Popular?: 0 = No, 1 = Yes')
```

It appears once like counts hits a certain point, the video will be popular.


```{r}
arm::binnedplot( x=logit.reg$fitted, y=logit.reg$resid, xlab="Predicted Probabilities", main="Bin Resid vs. Pred Prob", col.int = FALSE)
```

The residuals appear evenly distributed over and under the line and does not feature any patterns. There is likely an outlier at roughly 0.5, 0.3 in the Bin residual vs Predicted Probabilities graph, but it is difficult to comment how exactly it has affected the data.


```{r}
1-pchisq(logit.reg$deviance, logit.reg$df.residual)
```

With a value of 1 from the chi squared test, it suggests that we cannot reject the null hypothesis of the logistic regression being a good fit for the data. The logistic regression seems like an appropriate model for this dataset.



## Conclusion


With a strong logistic model that follows logistic regression assumptions from the ads from 2000 to 2021, brands that are looking to make a popular commercial for Superbowls should look to show their product quickly, and to have a likeable ad on Youtube. However, it is worth noting that the demarcation line for what consitutes a popular commercial was semi arbitrarily selected at a value of 200,000 views, and a shift up or down on that valuecould have easily shifted the significance and specific values of the logistic model that was generated.
