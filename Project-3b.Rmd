---
title: SuperBowl Commercial Popularity Factors
author: "John Roeder, Chris He, Malakai Vetock"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
Date Created: Tuesday, 2024/04/23, 14:45
Date Modified: Thursday, 2024/04/25, 14:44
class: MTH362 Statistical Modeling
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE, echo = FALSE) 
```

## Introduction

The dataset comes from FiveThirtyEight, a statistics focused news website that reported on sports, politics, and science.It contains the information of 233 ads from 10 brands that aired the most ad spots in 21 Superbowls ranging from 2000 to 2021 on the social media platform known as YouTube. The research question that is being investigated is "What factors may lead to a popular commercial during the super bowl? Given that the Superbowl is one of the most watched events each year with the most expensive ad slots, it makes sense to investigate factors that impact popularity and perception of money spent on ads during the Superbowl.

```{r}
library(tidyverse)
library(tidyr)
library(leaps)
youtube <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-03-02/youtube.csv')
```

The dataset contains 247 observations of 25 variables. A derived boolean variable called `popular` was created using `view_count`, using an arbitrarily selected limit of 200,000 as a marker for a popular commercial video, served as the response variable. As commercials ran during the Superbowl are expensive, it makes sense to set the standards of `popular` higher rather than lower. `popular` will be the response variable investigated in this project.

```{r}
# preprocessing 
num_columns <- ncol(youtube)

#get col names 
#also check for nans
columns =colnames(youtube)
nas <- colSums(is.na(youtube))

#find cols w nans
cols_w_nas <- names(nas[nas > 0])

#get the total missing for each col and print
missing_counts <- colSums(is.na(youtube))
```

All Nan cell values were checked. Some of them had missing values, which were filled with the median value of each column. Given that the dataset size was relatively small to begin with, it did not feel fitting to further reduce the dataset by dropping the entire row of data that contained a Nan.

```{r}

#for numeric cols w emptys
numeric_cols <- c("like_count", "dislike_count", "favorite_count", "comment_count", "view_count")
for (col in numeric_cols) {
  median_value <- median(youtube[[col]], na.rm = TRUE)  
  youtube[[col]][is.na(youtube[[col]])] <- median_value 
}

youtube <- youtube[!is.na(youtube$published_at), ]



#check again to see if fixes worked. 
missing_counts <- colSums(is.na(youtube))
```

```{r}
#change to binary features
#remove the unused features

youtube <- youtube %>%
  mutate_if(is.logical, as.integer)

youtube <- subset(youtube, select = -c(brand,
                                  superbowl_ads_dot_com_url,
                                  youtube_url,
                                  id,
                                  kind,
                                  etag,
                                  published_at,
                                  title,
                                  description,
                                  thumbnail,
                                  channel_title,
                                  category_id, 
                                  favorite_count))
youtube$popular<- with(youtube,ifelse(view_count>200000,1,0))

```

The last step of data preprocessing is removing the unwanted columns. Columns like superbowl_ads_dot_com_url, youtube_url, id,kind, etag, published_at, title, description, thumbnail, channel_title, category_id, and favorite_count are not necessary for the goal of predicting what makes a popular superbowl ad. These columns were mostly text data that would not work for a logistic regression model.

## EDA

```{r}
#EDA plots 


year_viewcount <- youtube %>%
  group_by(year) %>%
  summarise(avg_view_count = mean(view_count))
#avg view count by year
ggplot(year_viewcount, aes(x = as.factor(year), y = avg_view_count)) +
  geom_bar(stat = "identity", fill = "lightgreen", color = "black") +
  labs(title = "AVG View Count by Year",
       x = "Year",
       y = "AVG View Count") +
  theme_minimal()+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


commercial_viewcount <- youtube %>%
  summarise(
    funny_viewcount = sum(view_count * funny),
    show_product_quickly_viewcount = sum(view_count * show_product_quickly),
    patriotic_viewcount = sum(view_count * patriotic),
    celebrity_viewcount = sum(view_count * celebrity),
    danger_viewcount = sum(view_count * danger),
    animals_viewcount = sum(view_count * animals),
    use_sex_viewcount = sum(view_count * use_sex)
  ) %>%
  pivot_longer(cols = everything(), 
               names_to = "commercial_type", values_to = "total_viewcount")


ggplot(commercial_viewcount, aes(x = commercial_type, y = total_viewcount)) +
  geom_bar(stat = "identity", fill = "skyblue", color = "black") +
  labs(title = "Total View by commercial type",
       x = "Commercial type",
       y = "total view count") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))


ggplot(data=youtube,aes(x=popular))+geom_bar(fill= "skyblue")+labs(title="Popularity Count plot")


```

The above plot shows the average viewership of superbowl ads by year. 2012 had the highest average viewership followed by 2017 and 2020. This provides a visual of the relative increase in average viewership per superbowl ad. The total view by commercial type plot shows the total view counts across the different types of commercials. It can clearly be seen that the funny commercials and commercials that show products quickly have performed well in the past based on total view counts. Lastly the popular plot shows the counts of popular and unpopular superbowl commercials determined by our cutoff value of greater than 200,000 views. There are disproportionately more unpopular commercials as opposed to popular commercials per our metric.

## Model

Given that the response variable is binary, it made sense to settle on a logistic regression model

```{r}
logit.reg<- glm(popular ~ funny+ show_product_quickly+patriotic+celebrity+danger+animals+use_sex+like_count+dislike_count,data=youtube)
summary(logit.reg)


#check for multicollinearity
car::vif(logit.reg)
```

Like count and show product quickly were the only two significant predictors in the model with a p value below 0.05, resulting in the following model.

Assessing the variance inflation factors to check for multicollinearity, it can be seen that this dataset does not exhibit multicollinearity. All of the vif values are within a reasonable range being under 5 to indicate that there is no multicollinearity.

$logit(p)=0.1195+1.275\text{show_product_quickly}+8.454e^{-06}\text{like_count}$

```{r}
logit.reg2<- glm(popular ~show_product_quickly+like_count,data=youtube)
summary(logit.reg2)
```

$$
logit(p)=0.1221+0.1100\text{show_product_quickly}+5.443e^{-06}\text{like_count}
$$

If a commercial shows their product quickly, odds of popularity increase by $e^{0.1221}=1.12986708284$

Per a unit increase in like count, the odds of popularity increase by $e^{5.443e^{-06}}=1.00000544201$

## Analysis

```{r}
ggplot(data=youtube,aes(x=like_count,y=popular))+geom_jitter(alpha=0.5, height=0.1) + geom_smooth(method='glm', method.args=list(family='binomial'), se=TRUE) + labs(x='like_count', y='Popular?: 0 = No, 1 = Yes')+labs(title="Model plot")
```

It appears once the feature like counts hits a certain point, the video will be popular.

```{r}
arm::binnedplot( x=logit.reg$fitted, y=logit.reg$resid, xlab="Predicted Probabilities", main="Bin Resid vs. Pred Prob", col.int = FALSE)
```

The residuals appear evenly distributed over and under the line and does not feature any patterns. There is likely an outlier at roughly 0.5, 0.3 in the Bin residual vs Predicted Probabilities graph, but it is difficult to comment how exactly it has affected the data.

```{r}
1-pchisq(logit.reg$deviance, logit.reg$df.residual)
```

With a value of 1 from the chi squared test, it suggests that we cannot reject the null hypothesis of the logistic regression being a good fit for the data. The logistic regression seems like an appropriate model for this dataset.

## Conclusion

With a strong logistic model that follows logistic regression assumptions from the ads from 2000 to 2021, brands that are looking to make a popular commercial for Superbowls should look to show their product quickly, and to have a likeable ad on Youtube. However, it is worth noting that the demarcation line for what consitutes a popular commercial was semi arbitrarily selected at a value of 200,000 views, and a shift up or down on that value could have easily shifted the significance and specific values of the logistic model that was generated. Further analysis into what caused the one outlier or a dataset that includes the Superbowl advertisements since 2021 would increase the accuracy of the logistic regression model. Adjusting raw like count into a like/dislike ratio would likely give a better interpretation of results as raw like count may be skewed towards already successful videos
